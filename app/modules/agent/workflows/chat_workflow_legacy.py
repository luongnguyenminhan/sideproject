"""
Complete LangGraph workflow for chat system v·ªõi RAG capabilities
Production-ready implementation cho MoneyEZ financial assistant
"""

import os
from datetime import datetime, timezone
from typing import Dict, List, Optional, Annotated, TypedDict, Any
import logging

from dotenv import load_dotenv
from langchain_core.messages import (
	SystemMessage,
	HumanMessage,
	AIMessage,
	BaseMessage,
	ToolMessage,
)
from langchain_core.tools import BaseTool
from langchain_core.documents import Document
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.errors import NodeInterrupt
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver

logger = logging.getLogger(__name__)

# ==============================================
# 1. STATE DEFINITION
# ==============================================


class AgentState(TypedDict):
	"""State definition cho LangGraph workflow"""

	messages: Annotated[List[BaseMessage], add_messages]
	rag_context: Optional[List[str]]
	queries: Optional[List[str]]
	retrieved_docs: Optional[List[Document]]
	need_rag: Optional[bool]


# ==============================================
# 2. UTILITY FUNCTIONS
# ==============================================


def get_message_text(msg: BaseMessage) -> str:
	"""Extract text t·ª´ various message formats"""
	if hasattr(msg, 'content'):
		return str(msg.content)
	elif isinstance(msg, dict):
		return str(msg.get('content', ''))
	return str(msg)


def format_docs(docs: List[Document]) -> str:
	"""Format documents as XML cho prompts"""
	if not docs:
		return ''

	formatted = []
	for i, doc in enumerate(docs):
		content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
		formatted.append(f'<document_{i + 1}>\n{content}\n</document_{i + 1}>')

	return '\n\n'.join(formatted)


# ==============================================
# 3. RAG NODE FUNCTIONS
# ==============================================


async def should_use_rag(state: AgentState, config: Dict) -> Dict[str, Any]:
	"""
	Decision node: X√°c ƒë·ªãnh c√≥ c·∫ßn s·ª≠ d·ª•ng RAG kh√¥ng
	"""
	print(f'üîç [RAG Decision] Starting RAG analysis...')

	try:
		# Ki·ªÉm tra config setting
		use_rag_setting = config.get('configurable', {}).get('use_rag', True)
		if not use_rag_setting:
			print(f'üìù [RAG Decision] RAG disabled in config')
			return {'need_rag': False, 'queries': []}

		# L·∫•y user message cu·ªëi c√πng
		messages = state.get('messages', [])
		if not messages:
			print(f'üìù [RAG Decision] No messages found')
			return {'need_rag': False, 'queries': []}

		last_user_msg = None
		for msg in reversed(messages):
			if isinstance(msg, HumanMessage):
				last_user_msg = get_message_text(msg)
				break

		if not last_user_msg:
			print(f'üìù [RAG Decision] No user message found')
			return {'need_rag': False, 'queries': []}

		# Vietnamese financial keywords
		knowledge_keywords = [
			't√†i ch√≠nh',
			'th√¥ng tin',
			'gi·∫£i th√≠ch',
			'l√† g√¨',
			'ƒë·ªãnh nghƒ©a',
			'kh√°i ni·ªám',
			'c√°ch',
			'l√†m sao',
			't∆∞ v·∫•n',
			'n√™n',
			'h∆∞·ªõng d·∫´n',
			'quy ƒë·ªãnh',
			'lu·∫≠t',
			'ch√≠nh s√°ch',
			'so s√°nh',
			'kh√°c nhau',
			'ƒë·∫ßu t∆∞',
			'ti·∫øt ki·ªám',
			'ng√¢n h√†ng',
			't√≠n d·ª•ng',
			'b·∫£o hi·ªÉm',
			'thu·∫ø',
			'l√£i su·∫•t',
			'c·ªï phi·∫øu',
			'tr√°i phi·∫øu',
			'qu·ªπ ƒë·∫ßu t∆∞',
		]

		# Ph√¢n t√≠ch message
		need_rag = any(keyword in last_user_msg.lower() for keyword in knowledge_keywords)

		print(f"üìù [RAG Decision] Message: '{last_user_msg[:100]}...'")
		print(f'üìù [RAG Decision] Need RAG: {need_rag}')

		return {'need_rag': need_rag, 'queries': []}

	except Exception as e:
		logger.error(f'Error in should_use_rag: {str(e)}')
		print(f'‚ùå [RAG Decision] Error: {str(e)}')
		return {'need_rag': False, 'queries': []}


async def generate_query(state: AgentState, config: Dict) -> Dict[str, Any]:
	"""
	Generate optimized search queries cho RAG retrieval
	"""
	print(f'üîç [Query Generation] Starting query optimization...')

	try:
		messages = state.get('messages', [])
		if not messages:
			return {'queries': []}

		# L·∫•y user message cu·ªëi c√πng
		last_user_msg = None
		for msg in reversed(messages):
			if isinstance(msg, HumanMessage):
				last_user_msg = get_message_text(msg)
				break

		if not last_user_msg:
			return {'queries': []}

		# Remove filler words
		filler_words = [
			'cho t√¥i',
			'gi√∫p t√¥i',
			'l√†m ∆°n',
			'xin vui l√≤ng',
			'b·∫°n c√≥ th·ªÉ',
			't√¥i mu·ªën',
			't√¥i c·∫ßn',
			'xin',
			'h√£y',
			'th√¥ng tin v·ªÅ',
		]

		optimized_query = last_user_msg.lower()
		for filler in filler_words:
			optimized_query = optimized_query.replace(filler, '').strip()

		# Add financial terms n·∫øu t√¨m th·∫•y
		financial_terms = [
			'ƒë·∫ßu t∆∞',
			'ti·∫øt ki·ªám',
			'ng√¢n h√†ng',
			't√≠n d·ª•ng',
			'b·∫£o hi·ªÉm',
			'thu·∫ø',
			'l√£i su·∫•t',
			'c·ªï phi·∫øu',
			'tr√°i phi·∫øu',
			'qu·ªπ ƒë·∫ßu t∆∞',
		]

		keyword_query = optimized_query
		for term in financial_terms:
			if term in optimized_query:
				keyword_query += f' {term}'

		queries = [optimized_query.strip(), keyword_query.strip()]
		queries = [q for q in queries if q]  # Remove empty queries

		print(f"üìù [Query Generation] Original: '{last_user_msg[:100]}...'")
		print(f'üìù [Query Generation] Optimized queries: {queries}')

		return {'queries': queries}

	except Exception as e:
		logger.error(f'Error in generate_query: {str(e)}')
		print(f'‚ùå [Query Generation] Error: {str(e)}')
		return {'queries': []}


async def retrieve_knowledge(state: AgentState, config: Dict) -> Dict[str, Any]:
	"""
	Retrieve knowledge t·ª´ vector database
	"""
	print(f'üîç [Knowledge Retrieval] Starting document retrieval...')

	try:
		queries = state.get('queries', [])
		if not queries:
			print(f'üìù [Knowledge Retrieval] No queries to process')
			return {'rag_context': [], 'retrieved_docs': []}

		# Mock retriever function - Replace v·ªõi actual QdrantService integration
		def mock_retrieve_docs(query: str) -> List[Document]:
			"""Placeholder cho vector DB integration"""
			# Trong production, integrate v·ªõi QdrantService:
			# from app.modules.agent.services.qdrant_service import QdrantService
			# qdrant = QdrantService(db)
			# results = qdrant.search_documents(query, collection_name, top_k=5)

			mock_docs = [
				Document(
					page_content=f'Mock financial content cho query: {query}. Th√¥ng tin v·ªÅ t√†i ch√≠nh c√° nh√¢n v√† ƒë·∫ßu t∆∞.',
					metadata={'source': 'mock_source', 'score': 0.9},
				),
				Document(
					page_content=f'H∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ {query} trong lƒ©nh v·ª±c ng√¢n h√†ng v√† t√†i ch√≠nh.',
					metadata={'source': 'mock_guide', 'score': 0.8},
				),
			]
			return mock_docs

		# Process t·∫•t c·∫£ queries
		all_docs = []
		for query in queries:
			docs = mock_retrieve_docs(query)
			all_docs.extend(docs)

		# Deduplicate v√† rank documents
		unique_docs = []
		seen_content = set()

		for doc in all_docs:
			content = doc.page_content
			if content not in seen_content:
				seen_content.add(content)
				unique_docs.append(doc)

		# Sort by relevance score n·∫øu available
		unique_docs.sort(key=lambda x: x.metadata.get('score', 0), reverse=True)

		# Limit to top documents
		top_docs = unique_docs[:5]
		rag_context = [doc.page_content for doc in top_docs]

		print(f'üìù [Knowledge Retrieval] Retrieved {len(top_docs)} documents')
		for i, doc in enumerate(top_docs):
			print(f'üìÑ [Doc {i + 1}] {doc.page_content[:100]}...')

		return {'rag_context': rag_context, 'retrieved_docs': top_docs}

	except Exception as e:
		logger.error(f'Error in retrieve_knowledge: {str(e)}')
		print(f'‚ùå [Knowledge Retrieval] Error: {str(e)}')
		return {'rag_context': [], 'retrieved_docs': []}


# ==============================================
# 4. AGENT FUNCTIONS
# ==============================================


async def call_model(state: AgentState, config: Dict) -> Dict[str, Any]:
	"""
	Main agent function: Call LLM v·ªõi optional RAG context
	"""
	print(f'ü§ñ [Agent] Starting model invocation...')

	try:
		# Get system prompt t·ª´ config ho·∫∑c default
		default_system_prompt = """
        B·∫°n l√† MoneyEZ AI Assistant - tr·ª£ l√Ω t√†i ch√≠nh th√¥ng minh v√† th√¢n thi·ªán.
        
        Nhi·ªám v·ª•:
        ‚Ä¢ H·ªó tr·ª£ t∆∞ v·∫•n t√†i ch√≠nh c√° nh√¢n t·∫°i Vi·ªát Nam
        ‚Ä¢ Cung c·∫•p th√¥ng tin v·ªÅ ng√¢n h√†ng, ƒë·∫ßu t∆∞, ti·∫øt ki·ªám
        ‚Ä¢ Gi·∫£i th√≠ch c√°c kh√°i ni·ªám t√†i ch√≠nh m·ªôt c√°ch d·ªÖ hi·ªÉu
        ‚Ä¢ ƒê∆∞a ra l·ªùi khuy√™n ph√π h·ª£p v·ªõi lu·∫≠t ph√°p Vi·ªát Nam
        
        Nguy√™n t·∫Øc:
        ‚Ä¢ Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
        ‚Ä¢ Th√¥ng tin ch√≠nh x√°c, c·∫≠p nh·∫≠t
        ‚Ä¢ Gi·∫£i th√≠ch ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu  
        ‚Ä¢ Kh√¥ng ƒë∆∞a ra l·ªùi khuy√™n ƒë·∫ßu t∆∞ c·ª• th·ªÉ
        ‚Ä¢ Khuy·∫øn kh√≠ch tham kh·∫£o chuy√™n gia khi c·∫ßn
        """

		system_prompt = config.get('system_prompt', default_system_prompt)

		# Enhance v·ªõi RAG context n·∫øu c√≥
		rag_context = state.get('rag_context', [])
		if rag_context:
			context_text = '\n\n'.join(rag_context)
			enhanced_prompt = f"""
            {system_prompt}
            
            === TH√îNG TIN THAM KH·∫¢O ===
            {context_text}
            
            S·ª≠ d·ª•ng th√¥ng tin tr√™n ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c v√† chi ti·∫øt h∆°n.
            """
			system_prompt = enhanced_prompt
			print(f'üìù [Agent] Enhanced prompt v·ªõi RAG context ({len(rag_context)} docs)')

		# Setup model
		model_config = config.get('model_config', {})
		model = ChatGoogleGenerativeAI(
			model=model_config.get('model', 'gemini-2.0-flash'),
			temperature=model_config.get('temperature', 0),
			google_api_key=model_config.get('api_key', os.getenv('GOOGLE_API_KEY')),
		)

		# Create prompt template
		prompt = ChatPromptTemplate.from_messages([
			SystemMessage(content=system_prompt),
			MessagesPlaceholder(variable_name='messages'),
		])

		# Get available tools
		tools = get_tools(config)
		if tools:
			model = model.bind_tools(tools)
			print(f'üìù [Agent] Model bound v·ªõi {len(tools)} tools')

		# Create chain v√† invoke
		chain = prompt | model

		# Add current timestamp cho context
		current_time = datetime.now(timezone.utc).isoformat()

		# Prepare messages v·ªõi timestamp context
		messages = state.get('messages', [])
		if messages and isinstance(messages[-1], HumanMessage):
			# Add timestamp context
			last_msg = messages[-1]
			enhanced_content = f'{last_msg.content}\n\n[Th·ªùi gian hi·ªán t·∫°i: {current_time}]'
			messages[-1] = HumanMessage(content=enhanced_content)

		response = await chain.ainvoke({'messages': messages})

		print(f'ü§ñ [Agent] Model response generated successfully')
		print(f"üìù [Agent] Response preview: '{str(response.content)[:100]}...'")

		return {'messages': [response]}

	except Exception as e:
		logger.error(f'Error in call_model: {str(e)}')
		print(f'‚ùå [Agent] Error: {str(e)}')

		# Fallback response
		fallback_msg = AIMessage(content='Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. Vui l√≤ng th·ª≠ l·∫°i sau.')
		return {'messages': [fallback_msg]}


def should_continue(state: AgentState) -> str:
	"""
	Conditional edge: Ki·ªÉm tra c√≥ tool calls kh√¥ng
	"""
	messages = state.get('messages', [])
	if not messages:
		return END

	last_message = messages[-1]

	# Ki·ªÉm tra tool calls
	if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
		print(f'üîß [Routing] Tool calls detected: {len(last_message.tool_calls)}')
		return 'tools'

	print(f'üèÅ [Routing] No tool calls, ending conversation')
	return END


async def run_tools(state: AgentState, config: Dict, **kwargs) -> Dict[str, Any]:
	"""
	Execute tools v√† return response
	"""
	print(f'üîß [Tools] Starting tool execution...')

	try:
		tools = get_tools(config)
		if not tools:
			print(f'‚ùå [Tools] No tools available')
			return {'messages': []}

		tool_node = ToolNode(tools)
		result = await tool_node.ainvoke(state, config, **kwargs)

		print(f'‚úÖ [Tools] Tool execution completed')
		return result

	except Exception as e:
		logger.error(f'Error in run_tools: {str(e)}')
		print(f'‚ùå [Tools] Error: {str(e)}')

		error_msg = ToolMessage(content=f'Tool execution failed: {str(e)}', tool_call_id='error')
		return {'messages': [error_msg]}


# ==============================================
# 5. TOOL SUPPORT
# ==============================================


class FrontendTool(BaseTool):
	"""
	Tool cho frontend-rendered functionality
	"""

	name: str
	description: str

	def _run(self, *args, **kwargs):
		"""Raise interrupt cho frontend handling"""
		raise NodeInterrupt(f'Frontend tool: {self.name}')

	async def _arun(self, *args, **kwargs):
		"""Async version"""
		raise NodeInterrupt(f'Frontend tool: {self.name}')


def get_tool_defs(config: Dict) -> List[Dict]:
	"""
	Get tool definitions t·ª´ config
	"""
	frontend_tools = config.get('frontend_tools', [])
	backend_tools = config.get('backend_tools', [])

	all_tools = []
	all_tools.extend(frontend_tools)
	all_tools.extend(backend_tools)

	return all_tools


def get_tools(config: Dict) -> List[BaseTool]:
	"""
	Create tool instances cho ToolNode
	"""
	tool_defs = get_tool_defs(config)
	tools = []

	for tool_def in tool_defs:
		if tool_def.get('type') == 'frontend':
			tool = FrontendTool(
				name=tool_def.get('name', 'unknown_tool'),
				description=tool_def.get('description', 'Frontend tool'),
			)
			tools.append(tool)
		elif tool_def.get('type') == 'backend':
			# Add backend tool implementations n·∫øu c·∫ßn
			pass

	return tools


# ==============================================
# 6. WORKFLOW GRAPH CONSTRUCTION
# ==============================================


def create_workflow() -> StateGraph:
	"""
	Create complete LangGraph workflow
	"""
	print(f'üèóÔ∏è [Workflow] Building LangGraph workflow...')

	# Initialize workflow
	workflow = StateGraph(AgentState)

	# Add nodes
	workflow.add_node('rag_decision', should_use_rag)
	workflow.add_node('generate_query', generate_query)
	workflow.add_node('retrieve', retrieve_knowledge)
	workflow.add_node('agent', call_model)
	workflow.add_node('tools', run_tools)

	# Set entry point
	workflow.set_entry_point('rag_decision')

	# Add conditional edges cho RAG flow
	def route_rag_decision(state: AgentState) -> str:
		need_rag = state.get('need_rag', False)
		return 'use_rag' if need_rag else 'skip_rag'

	workflow.add_conditional_edges(
		'rag_decision',
		route_rag_decision,
		{'use_rag': 'generate_query', 'skip_rag': 'agent'},
	)

	# RAG pipeline edges
	workflow.add_edge('generate_query', 'retrieve')
	workflow.add_edge('retrieve', 'agent')

	# Tool handling edges
	workflow.add_conditional_edges('agent', should_continue, {'tools': 'tools', END: END})
	workflow.add_edge('tools', 'agent')

	print(f'‚úÖ [Workflow] LangGraph workflow created successfully')
	return workflow


# ==============================================
# 7. FINAL WORKFLOW COMPILATION
# ==============================================

# Create workflow instance
workflow = create_workflow()

# Setup memory checkpointer
memory = MemorySaver()

# Compile graph v·ªõi memory
assistant_ui_graph = workflow.compile(checkpointer=memory)

print(f'üöÄ [System] MoneyEZ LangGraph Assistant ready!')
print(f'üìä [System] Workflow nodes: {len(workflow.nodes)}')
print(f'üîó [System] Memory persistence: Enabled')
print(f'üîß [System] RAG capabilities: Enabled')
print(f'ü§ñ [System] Model: gemini-2.0-flash')
