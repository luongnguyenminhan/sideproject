"""
Question Composer Tool for Chat Agent - Simplified Function-based Implementation
"""

import logging
import json
import asyncio
from typing import Dict, Any, Optional, List
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
from datetime import datetime

from app.modules.question_composer.repository.question_composer_repo import (
	QuestionComposerRepo,
)
from app.modules.question_composer.schemas.question_request import (
	QuestionGenerationRequest,
	AnalyzeUserProfileRequest,
)
from app.exceptions.exception import ValidationException

logger = logging.getLogger(__name__)


class QuestionComposerInput(BaseModel):
	"""Input schema for Question Composer tool"""

	action: str = Field(description="'generate' ho·∫∑c 'analyze'")
	user_id: Optional[str] = Field(None, description='ID c·ªßa user')
	session_id: Optional[str] = Field(None, description='Session ID')
	existing_user_data: Optional[Dict[str, Any]] = Field(None, description='D·ªØ li·ªáu user hi·ªán t·∫°i')
	previous_questions: Optional[List[Dict[str, Any]]] = Field(None, description='C√¢u h·ªèi ƒë√£ h·ªèi')
	focus_areas: Optional[List[str]] = Field(None, description='Khu v·ª±c t·∫≠p trung')
	max_questions: int = Field(4, description='S·ªë c√¢u h·ªèi t·ªëi ƒëa')
	max_iterations: int = Field(5, description='S·ªë iteration t·ªëi ƒëa')


@tool('question_composer', args_schema=QuestionComposerInput, return_direct=False)
def question_composer_tool(
	action: str,
	user_id: Optional[str] = None,
	session_id: Optional[str] = None,
	existing_user_data: Dict[str, Any] = None,
	previous_questions: List[Dict[str, Any]] = None,
	focus_areas: List[str] = None,
	max_questions: int = 4,
	max_iterations: int = 5,
) -> str:
	"""
	‚ùì QUESTION COMPOSER TOOL - B·∫ÆT BU·ªòC cho CV, profile, c√¢u h·ªèi!

	üìã ∆ØU TI√äN S·ª¨ D·ª§NG TOOL N√ÄY KHI:
	‚úÖ User ƒë·ªÅ c·∫≠p ƒë·∫øn "CV", "profile", "resume", "h·ªì s∆°"
	‚úÖ User h·ªèi v·ªÅ "compose questions", "t·∫°o c√¢u h·ªèi"
	‚úÖ User mu·ªën "analyze", "ph√¢n t√≠ch" profile/CV
	‚úÖ User c·∫ßn "interview questions", "c√¢u h·ªèi ph·ªèng v·∫•n"
	‚úÖ User h·ªèi v·ªÅ "career development", "ph√°t tri·ªÉn s·ª± nghi·ªáp"
	‚úÖ User upload CV ho·∫∑c n√≥i v·ªÅ kinh nghi·ªám l√†m vi·ªác
	‚úÖ User mu·ªën "c·∫£i thi·ªán CV", "ho√†n thi·ªán profile"
	‚úÖ User h·ªèi v·ªÅ "skills assessment", "ƒë√°nh gi√° k·ªπ nƒÉng"

	‚ö†Ô∏è LU√îN S·ª¨ D·ª§NG tool n√†y cho m·ªçi ch·ªß ƒë·ªÅ li√™n quan CV/profile!

	üõ†Ô∏è HAI CH·ª®C NƒÇNG CH√çNH:
	1Ô∏è‚É£ ANALYZE (action="analyze"): Ph√¢n t√≠ch ƒë·ªô ho√†n thi·ªán profile
	2Ô∏è‚É£ GENERATE (action="generate"): T·∫°o c√¢u h·ªèi th√¥ng minh

	Args:
	    action: "analyze" ho·∫∑c "generate" (B·∫ÆT BU·ªòC)
	    user_id: ID ng∆∞·ªùi d√πng (optional)
	    session_id: ID session (optional)
	    existing_user_data: Dict th√¥ng tin user hi·ªán c√≥
	    previous_questions: List c√¢u h·ªèi ƒë√£ h·ªèi
	    focus_areas: List khu v·ª±c focus (optional)
	    max_questions: S·ªë c√¢u h·ªèi t·ªëi ƒëa (default: 4)
	    max_iterations: S·ªë iteration t·ªëi ƒëa (default: 5)

	Returns:
	    JSON string v·ªõi k·∫øt qu·∫£ analysis ho·∫∑c questions
	"""
	print(f'üéØ [QuestionComposer] Tool called with action: {action}')

	try:
		# Normalize inputs
		existing_user_data = existing_user_data or {}
		previous_questions = previous_questions or []
		focus_areas = focus_areas or []

		print(f'üìä [QuestionComposer] Processing - User: {user_id}, Session: {session_id}')
		print(f'üìã [QuestionComposer] Data keys: {list(existing_user_data.keys())}')
		print(f'‚ùì [QuestionComposer] Previous questions: {len(previous_questions)}')

		# Execute action using asyncio
		if action == 'analyze':
			result = asyncio.run(_analyze_profile_async(existing_user_data, previous_questions))
		elif action == 'generate':
			result = asyncio.run(
				_generate_questions_async(
					user_id,
					session_id,
					existing_user_data,
					previous_questions,
					focus_areas,
					max_questions,
					max_iterations,
				)
			)
			print(f'üìà [QuestionComposer] Generated {json.loads(result)} questions')
		else:
			raise ValidationException(f"Invalid action: {action}. Use 'generate' or 'analyze'")

		print(f'‚úÖ [QuestionComposer] Tool execution completed successfully')
		return result

	except Exception as e:
		print(f'üí• [QuestionComposer] Error: {str(e)}')
		error_data = {
			'status': 'error',
			'type': f'question_{action}',
			'error': str(e),
		}
		return json.dumps(error_data, ensure_ascii=False)


async def _analyze_profile_async(existing_user_data: Dict[str, Any], previous_questions: List[Dict[str, Any]]) -> str:
	"""Real profile analysis using QuestionComposerRepo"""
	print(f'üîç [QuestionComposer] Starting REAL profile analysis')

	# Input validation
	if not existing_user_data:
		return json.dumps(
			{
				'status': 'error',
				'type': 'profile_analysis',
				'error': 'No user data provided for analysis',
				'suggestion': 'Please provide user profile data to analyze',
			},
			ensure_ascii=False,
		)

	try:
		# Import the real components
		from app.modules.question_composer.repository.question_composer_repo import (
			QuestionComposerRepo,
		)
		from app.modules.question_composer.schemas.question_request import (
			AnalyzeUserProfileRequest,
		)
		from app.core.database import get_db

		# Get database session
		db_session = None
		try:
			db_session = next(get_db())
			print(f'‚úÖ [QuestionComposer] Database session acquired')
		except Exception as db_e:
			print(f'‚ö†Ô∏è [QuestionComposer] Database session failed, using fallback: {str(db_e)}')
			# Fallback to mock analysis if DB not available
			return await _fallback_analysis(existing_user_data, previous_questions)

		# Create repository instance
		repo = QuestionComposerRepo(db_session)
		print(f'‚úÖ [QuestionComposer] Repository instance created')

		# Prepare request
		analyze_request = AnalyzeUserProfileRequest(user_profile=existing_user_data, previous_questions=previous_questions or [])
		print(f'üìã [QuestionComposer] Analysis request prepared with {len(existing_user_data)} data fields')

		# Call real analysis
		print(f'üöÄ [QuestionComposer] Calling real analysis service')
		analysis_response = await repo.analyze_user_profile(analyze_request)
		print(f'‚úÖ [QuestionComposer] Analysis completed successfully')
		print(f'üîç [QuestionComposer] Response type: {type(analysis_response).__name__}')
		print(f'üîç [QuestionComposer] Response attributes: {dir(analysis_response)}')

		# Convert response to tool format - Use safe serialization
		missing_areas = _safe_serialize_object(analysis_response.missing_areas, [])
		suggested_focus = _safe_serialize_object(analysis_response.suggested_focus, [])
		next_steps = _safe_serialize_object(getattr(analysis_response, 'next_steps', 'Continue with profile completion'), 'Continue with profile completion')
		summary = _safe_serialize_object(getattr(analysis_response, 'summary', 'Profile analysis completed'), 'Profile analysis completed')
		print(f'üîç [QuestionComposer] Serialized analysis data')

		result_data = {
			'status': 'success',
			'type': 'profile_analysis',
			'analysis': {
				'completeness_score': round(float(_safe_serialize_object(analysis_response.completeness_score, 0.5)), 2),
				'should_continue': bool(_safe_serialize_object(analysis_response.should_continue, True)),
				'missing_areas': missing_areas,
				'suggested_focus': suggested_focus,
				'data_quality': _safe_serialize_object(getattr(analysis_response, 'data_quality', 'good'), 'good'),
				'questions_analyzed': (len(previous_questions) if previous_questions else 0),
				'analysis_text': _safe_serialize_object(getattr(analysis_response, 'analysis_text', 'Analysis completed'), 'Analysis completed'),
			},
			'recommendations': {
				'should_ask_more': bool(_safe_serialize_object(analysis_response.should_continue, True)),
				'priority_areas': suggested_focus,
				'next_steps': next_steps,
				'summary': summary,
			},
			'metadata': {
				'analysis_timestamp': datetime.now().isoformat(),
				'data_keys_found': list(existing_user_data.keys()),
				'total_data_points': len(existing_user_data),
				'service_used': 'real_question_composer_repo',
				'response_type': str(type(analysis_response).__name__),
			},
		}

		print(f'‚úÖ [QuestionComposer] Real analysis completed with score: {analysis_response.completeness_score:.2f}')
		return json.dumps(result_data, ensure_ascii=False, indent=2)

	except Exception as e:
		print(f'üí• [QuestionComposer] Real analysis error: {str(e)}')
		print(f'üîÑ [QuestionComposer] Falling back to mock analysis')
		# Fallback to mock if real analysis fails
		return await _fallback_analysis(existing_user_data, previous_questions)
	finally:
		if db_session:
			db_session.close()
			print(f'üîí [QuestionComposer] Database session closed')


async def _generate_questions_async(
	user_id: Optional[str],
	session_id: Optional[str],
	existing_user_data: Dict[str, Any],
	previous_questions: List[Dict[str, Any]],
	focus_areas: List[str],
	max_questions: int,
	max_iterations: int,
) -> str:
	"""Real question generation using QuestionComposerRepo"""
	print(f'üé® [QuestionComposer] Starting REAL question generation')

	try:
		# Import the real components
		from app.modules.question_composer.repository.question_composer_repo import (
			QuestionComposerRepo,
		)
		from app.modules.question_composer.schemas.question_request import (
			QuestionGenerationRequest,
		)
		from app.core.database import get_db

		# Get database session
		db_session = None
		try:
			db_session = next(get_db())
			print(f'‚úÖ [QuestionComposer] Database session acquired')
		except Exception as db_e:
			print(f'‚ö†Ô∏è [QuestionComposer] Database session failed, using fallback: {str(db_e)}')
			# Fallback to mock generation if DB not available
			return await _fallback_generation(
				user_id,
				session_id,
				existing_user_data,
				previous_questions,
				focus_areas,
				max_questions,
			)

		# Create repository instance
		repo = QuestionComposerRepo(db_session)
		print(f'‚úÖ [QuestionComposer] Repository instance created')

		# Prepare request
		generation_request = QuestionGenerationRequest(
			session_id=session_id,
			user_id=user_id,
			existing_user_data=existing_user_data,
			previous_questions=previous_questions or [],
			focus_areas=focus_areas or [],
			max_questions=max_questions,
			max_iterations=max_iterations,
		)
		print(f'üìã [QuestionComposer] Generation request prepared with {len(existing_user_data)} data fields')

		# Call real generation
		print(f'üöÄ [QuestionComposer] Calling real generation service')
		generation_response = await repo.generate_questions(generation_request)
		print(f'‚úÖ [QuestionComposer] Generation completed successfully')
		print(f'üîç [QuestionComposer] Response type: {type(generation_response).__name__}')
		print(f'üîç [QuestionComposer] Response attributes: {dir(generation_response)}')

		# Convert response to tool format - Use safe serialization
		serialized_questions = _safe_serialize_list(generation_response.questions, [])
		next_focus_areas = _safe_serialize_object(generation_response.next_focus_areas, [])
		print(f'üîç [QuestionComposer] Serialized {len(serialized_questions)} questions')

		result_data = {
			'status': 'success',
			'type': 'question_generation',
			'session_info': {
				'session_id': _safe_serialize_object(generation_response.session_id, session_id),
				'current_iteration': _safe_serialize_object(getattr(generation_response, 'current_iteration', 1), 1),
				'total_questions_generated': _safe_serialize_object(getattr(generation_response, 'total_questions_generated', len(serialized_questions)), len(serialized_questions)),
			},
			'analysis': {
				'completeness_score': round(float(_safe_serialize_object(generation_response.completeness_score, 0.5)), 2),
				'should_continue': bool(_safe_serialize_object(generation_response.should_continue, True)),
				'next_focus_areas': next_focus_areas,
				'analysis_text': _safe_serialize_object(getattr(generation_response, 'analysis', 'Analysis completed'), 'Analysis completed'),
			},
			'questions': serialized_questions,
			'metadata': {
				'generation_timestamp': datetime.now().isoformat(),
				'service_used': 'real_question_composer_repo',
				'focus_areas_requested': focus_areas,
				'response_type': str(type(generation_response).__name__),
			},
		}

		print(f'‚úÖ [QuestionComposer] Real generation completed with {len(generation_response.questions)} questions')
		return json.dumps(result_data, ensure_ascii=False, indent=2)

	except Exception as e:
		print(f'üí• [QuestionComposer] Real generation error: {str(e)}')
		print(f'üîÑ [QuestionComposer] Falling back to mock generation')
		# Fallback to mock if real generation fails
		return await _fallback_generation(
			user_id,
			session_id,
			existing_user_data,
			previous_questions,
			focus_areas,
			max_questions,
		)
	finally:
		if db_session:
			db_session.close()
			print(f'üîí [QuestionComposer] Database session closed')


def get_question_composer_tool(db_session: Session):
	"""Factory function ƒë·ªÉ t·∫°o question composer tool v·ªõi db_session"""
	print(f'üè≠ [Factory] Creating simplified question composer tool')

	# Store db_session in global context or closure
	# For a more robust solution, you could use a closure or global variable
	global _global_db_session
	_global_db_session = db_session

	print(f'‚úÖ [Factory] Question composer tool created')
	return question_composer_tool


# Global variable to store db_session (not ideal but simple)
_global_db_session = None


async def _fallback_analysis(existing_user_data: Dict[str, Any], previous_questions: List[Dict[str, Any]]) -> str:
	"""Fallback mock analysis when real service fails"""
	print(f'üîÑ [QuestionComposer] Running fallback analysis')

	try:
		# Basic analysis logic
		data_keys = list(existing_user_data.keys())
		data_quality_score = min(1.0, len(data_keys) / 10)  # Normalize by 10 expected fields

		# Analyze previous questions
		questions_asked = len(previous_questions) if previous_questions else 0
		questions_impact = min(0.3, questions_asked * 0.05)  # Each question adds 5% up to 30%

		# Calculate overall completeness
		completeness_score = (data_quality_score * 0.7) + questions_impact

		# Determine missing areas based on common profile fields
		expected_areas = [
			'personal_info',
			'education',
			'experience',
			'skills',
			'goals',
			'preferences',
		]
		missing_areas = [area for area in expected_areas if area not in data_keys]

		# Create intelligent recommendations
		should_continue = completeness_score < 0.8 or len(missing_areas) > 0

		result_data = {
			'status': 'success',
			'type': 'profile_analysis',
			'analysis': {
				'completeness_score': round(completeness_score, 2),
				'should_continue': should_continue,
				'missing_areas': missing_areas,
				'suggested_focus': missing_areas[:3],  # Top 3 priorities
				'data_quality': ('good' if data_quality_score > 0.6 else 'needs_improvement'),
				'questions_analyzed': questions_asked,
				'analysis_text': (f'Profile hi·ªán t·∫°i c√≥ ƒëi·ªÉm ho√†n thi·ªán {round(completeness_score * 100)}%. ' + f'C·∫ßn b·ªï sung th√™m th√¥ng tin v·ªÅ: {", ".join(missing_areas[:3])}.' if missing_areas else 'Profile ƒë√£ kh√° ho√†n thi·ªán.'),
			},
			'recommendations': {
				'should_ask_more': should_continue,
				'priority_areas': missing_areas[:3] if missing_areas else [],
				'next_steps': ('T·∫°o c√¢u h·ªèi ƒë·ªÉ ho√†n thi·ªán profile' if should_continue else 'Profile ƒë√£ ƒë·ªß th√¥ng tin'),
				'summary': (f'C·∫ßn h·ªèi th√™m {3 - len(data_keys)} c√¢u h·ªèi ƒë·ªÉ c√≥ profile ho√†n thi·ªán.' if should_continue else 'Profile ƒë√£ ho√†n thi·ªán.'),
			},
			'metadata': {
				'analysis_timestamp': datetime.now().isoformat(),
				'data_keys_found': data_keys,
				'total_data_points': len(data_keys),
				'service_used': 'fallback_mock_analysis',
			},
		}

		print(f'‚úÖ [QuestionComposer] Fallback analysis completed with score: {completeness_score:.2f}')
		return json.dumps(result_data, ensure_ascii=False, indent=2)

	except Exception as e:
		print(f'üí• [QuestionComposer] Fallback analysis error: {str(e)}')
		error_result = {
			'status': 'error',
			'type': 'profile_analysis',
			'error': str(e),
			'fallback_recommendation': {
				'should_continue': True,
				'suggested_action': 'Continue with general questions',
			},
		}
		return json.dumps(error_result, ensure_ascii=False)


async def _fallback_generation(
	user_id: Optional[str],
	session_id: Optional[str],
	existing_user_data: Dict[str, Any],
	previous_questions: List[Dict[str, Any]],
	focus_areas: List[str],
	max_questions: int,
) -> str:
	"""Fallback mock generation when real service fails"""
	print(f'üîÑ [QuestionComposer] Running fallback generation')

	try:
		# Generate mock questions based on focus areas
		mock_questions = []

		# Base questions if no focus areas
		if not focus_areas:
			mock_questions = [
				{
					'id': 'q_1',
					'question': 'B·∫°n c√≥ kinh nghi·ªám l√†m vi·ªác v·ªõi c√¥ng ngh·ªá n√†o?',
					'type': 'multiple_choice',
					'data': {'options': ['Python', 'JavaScript', 'Java', 'C++', 'Kh√°c']},
					'required': True,
					'order': 1,
				},
				{
					'id': 'q_2',
					'question': 'M√¥ t·∫£ d·ª± √°n ·∫•n t∆∞·ª£ng nh·∫•t b·∫°n ƒë√£ th·ª±c hi·ªán?',
					'type': 'text_input',
					'required': True,
					'order': 2,
				},
				{
					'id': 'q_3',
					'question': 'B·∫°n mu·ªën ph√°t tri·ªÉn s·ª± nghi·ªáp theo h∆∞·ªõng n√†o?',
					'type': 'single_option',
					'data': {
						'options': [
							'Technical Leader',
							'Manager',
							'Specialist',
							'Entrepreneur',
						]
					},
					'required': True,
					'order': 3,
				},
				{
					'id': 'q_4',
					'question': 'Th√¥ng tin th√™m v·ªÅ h·ªçc v·∫•n v√† ch·ª©ng ch·ªâ',
					'type': 'sub_form',
					'data': {
						'fields': [
							{'name': 'degree', 'type': 'text', 'label': 'B·∫±ng c·∫•p'},
							{
								'name': 'university',
								'type': 'text',
								'label': 'Tr∆∞·ªùng h·ªçc',
							},
							{
								'name': 'year',
								'type': 'number',
								'label': 'NƒÉm t·ªët nghi·ªáp',
							},
						]
					},
					'required': False,
					'order': 4,
				},
			]
		else:
			# Generate questions based on focus areas
			for i, area in enumerate(focus_areas[:max_questions]):
				if area == 'skills':
					mock_questions.append({
						'id': f'q_{i + 1}',
						'question': f'B·∫°n c√≥ k·ªπ nƒÉng g√¨ trong lƒ©nh v·ª±c {area}?',
						'type': 'multiple_choice',
						'data': {
							'options': [
								'Beginner',
								'Intermediate',
								'Advanced',
								'Expert',
							]
						},
						'required': True,
						'order': i + 1,
					})
				elif area == 'experience':
					mock_questions.append({
						'id': f'q_{i + 1}',
						'question': f'Kinh nghi·ªám c·ªßa b·∫°n trong {area} nh∆∞ th·∫ø n√†o?',
						'type': 'text_input',
						'required': True,
						'order': i + 1,
					})
				else:
					mock_questions.append({
						'id': f'q_{i + 1}',
						'question': f'Chia s·∫ª th√¥ng tin v·ªÅ {area} c·ªßa b·∫°n',
						'type': 'text_input',
						'required': True,
						'order': i + 1,
					})

		result_data = {
			'status': 'success',
			'type': 'question_generation',
			'session_info': {
				'session_id': session_id or f'session_{user_id}',
				'current_iteration': 1,
				'total_questions_generated': len(mock_questions),
			},
			'analysis': {
				'completeness_score': 0.6,
				'should_continue': True,
				'next_focus_areas': focus_areas or ['skills', 'experience'],
				'analysis_text': 'Profile c·∫ßn th√™m th√¥ng tin ƒë·ªÉ ho√†n thi·ªán.',
			},
			'questions': mock_questions,
			'metadata': {
				'generation_timestamp': datetime.now().isoformat(),
				'service_used': 'fallback_mock_generation',
				'focus_areas_requested': focus_areas,
			},
		}

		print(f'‚úÖ [QuestionComposer] Fallback generation completed with {len(mock_questions)} questions')
		return json.dumps(result_data, ensure_ascii=False, indent=2)

	except Exception as e:
		print(f'üí• [QuestionComposer] Fallback generation error: {str(e)}')
		raise e


def _safe_serialize_object(obj, default_value=None):
	"""Safely serialize objects to JSON-compatible format"""
	try:
		if obj is None:
			return default_value

		# Handle Pydantic models
		if hasattr(obj, 'dict'):
			return obj.dict()

		# Handle dataclasses
		if hasattr(obj, '__dict__'):
			return obj.__dict__

		# Handle iterables (but not strings)
		if hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes)):
			return list(obj)

		# Handle basic types
		if isinstance(obj, (int, float, str, bool)):
			return obj

		# Convert to string as fallback
		return str(obj)

	except Exception as e:
		print(f'‚ö†Ô∏è [Serialization] Error serializing {type(obj)}: {str(e)}')
		return default_value or str(obj)


def _safe_serialize_list(obj_list, default_value=None):
	"""Safely serialize a list of objects"""
	if not obj_list:
		return default_value or []

	try:
		serialized_list = []
		for item in obj_list:
			serialized_item = _safe_serialize_object(item, {})
			serialized_list.append(serialized_item)
		return serialized_list
	except Exception as e:
		print(f'‚ö†Ô∏è [Serialization] Error serializing list: {str(e)}')
		return default_value or []


def test_serialization():
	"""Test serialization functions"""
	import json

	# Test with various objects
	test_cases = [
		{'name': 'dict', 'value': {'a': 1, 'b': 2}},
		{'name': 'list', 'value': [1, 2, 3]},
		{'name': 'string', 'value': 'test'},
		{'name': 'int', 'value': 42},
		{'name': 'float', 'value': 3.14},
		{'name': 'bool', 'value': True},
		{'name': 'none', 'value': None},
	]

	print('üß™ Testing serialization functions:')
	for case in test_cases:
		try:
			result = _safe_serialize_object(case['value'])
			json_str = json.dumps(result, ensure_ascii=False)
			print(f'‚úÖ {case["name"]}: {result} -> JSON OK')
		except Exception as e:
			print(f'‚ùå {case["name"]}: {e}')


# Uncomment to run test
# test_serialization()
