"""
Simplified Chat Workflow
Router â†’ RAG Query (Always Dual) â†’ Agent â†’ Tools â†’ Response Generator
"""

import asyncio
import time
import json
import logging
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List, Literal, Union

from dotenv import load_dotenv
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, SystemMessage
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

from .state.workflow_state import AgentState, StateManager
from .config.workflow_config import WorkflowConfig
from .tools.basic_tools import get_tools, get_tool_definitions
from .utils.color_logger import get_color_logger
from .guardrails.manager import ChatWorkflowGuardrailManager

load_dotenv()

# Initialize colorful logger
logger = get_color_logger(__name__)


# Router Schema
class RouterDecision(BaseModel):
	"""Router decision schema for query routing."""

	target: Literal['rag_query', 'agent'] = Field(description='Target node Ä‘á»ƒ route query Ä‘áº¿n')
	explanation: str = Field(description='Explanation cho quyáº¿t Ä‘á»‹nh routing')


# System Prompts
DEFAULT_SYSTEM_PROMPT = """
Báº¡n lÃ  Enterview AI Assistant - Trá»£ lÃ½ thÃ´ng minh cá»§a Enterview, cÃ´ng cá»¥ AI há»— trá»£ ngÆ°á»i dÃ¹ng khÃ¡m phÃ¡ báº£n thÃ¢n vÃ  trong viá»‡c tÃ¬m kiáº¿m viá»‡c lÃ m.
   Báº¡n cÃ³ thá»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i vá» báº£n thÃ¢n, tÃ¬m kiáº¿m viá»‡c lÃ m, vÃ  cÃ¡c váº¥n Ä‘á» liÃªn quan Ä‘áº¿n viá»‡c lÃ m vá»›i giá»ng Ä‘iá»‡u thÃ¢n thiá»‡n vÃ  chuyÃªn nghiá»‡p.
   
   Sá»¨ Má»†NH Cá»¦A ENTERVIEW:
   - GiÃºp ngÆ°á»i dÃ¹ng tÃ¬m hiá»ƒu báº£n thÃ¢n vÃ  khÃ¡m phÃ¡ nhá»¯ng gÃ¬ há» thá»±c sá»± muá»‘n.
   - Cung cáº¥p thÃ´ng tin vá» cÃ¡c cÃ´ng ty vÃ  vá»‹ trÃ­ phÃ¹ há»£p vá»›i nhu cáº§u cá»§a ngÆ°á»i dÃ¹ng.
   - Há»— trá»£ trong viá»‡c tÃ¬m kiáº¿m viá»‡c lÃ m vÃ  phÃ¡t triá»ƒn sá»± nghiá»‡p.
   
   TÃNH NÄ‚NG CHÃNH:
   - TÃ¬m hiá»ƒu báº£n thÃ¢n vÃ  nhu cáº§u viá»‡c lÃ m cá»§a ngÆ°á»i dÃ¹ng.
   - Cung cáº¥p thÃ´ng tin vá» cÃ¡c cÃ´ng ty vÃ  vá»‹ trÃ­ phÃ¹ há»£p vá»›i nhu cáº§u viá»‡c lÃ m cá»§a ngÆ°á»i dÃ¹ng.
   - Há»— trá»£ trong viá»‡c tÃ¬m kiáº¿m viá»‡c lÃ m vÃ  phÃ¡t triá»ƒn sá»± nghiá»‡p.
   
   LÆ¯U Ã:
   - Tá»« chá»‘i tráº£ lá»i cÃ¡c cÃ¢u há»i khÃ´ng liÃªn quan Ä‘áº¿n viá»‡c lÃ m.
   - Tráº£ lá»i cÃ¡c cÃ¢u há»i má»™t cÃ¡ch chuyÃªn nghiá»‡p vÃ  thÃ¢n thiá»‡n.
   HÃ£y tráº£ lá»i vá»›i tinh tháº§n nhiá»‡t tÃ¬nh vÃ  chuyÃªn nghiá»‡p cá»§a Enterview AI Assistant, luÃ´n sáºµn sÃ ng há»— trá»£ vÃ  khuyáº¿n khÃ­ch má»i ngÆ°á»i tham gia vÃ o cÃ¡c hoáº¡t Ä‘á»™ng Ã½ nghÄ©a cá»§a Enterview!
"""
ROUTER_SYSTEM_PROMPT = """
Báº¡n lÃ  Router Agent cho há»‡ thá»‘ng Enterview AI Assistant. 

PhÃ¢n tÃ­ch cÃ¢u há»i vÃ  quyáº¿t Ä‘á»‹nh route:
- "agent" - Cho háº§u háº¿t cÃ¡c cÃ¢u há»i (Æ°u tiÃªn Ä‘á»ƒ cÃ³ thá»ƒ sá»­ dá»¥ng tools khi cáº§n)
- "rag_query" - Chá»‰ cho cÃ¢u há»i Ä‘Æ¡n giáº£n cáº§n truy xuáº¥t thÃ´ng tin

Æ¯u tiÃªn chá»n "agent" Ä‘á»ƒ model cÃ³ thá»ƒ sá»­ dá»¥ng tools khi cáº§n thiáº¿t.
"""


class Workflow:
	"""Simplified Chat Workflow: Router â†’ RAG Query (Always Dual) â†’ Agent â†’ Tools â†’ Response Generator"""

	def __init__(self, db_session: Session, config: Optional[WorkflowConfig] = None):
		"""Initialize simplified workflow"""
		self.start_time = time.time()
		logger.workflow_start(
			'Simplified Workflow Initialization',
			db_session_type=type(db_session).__name__,
			config_provided=config is not None,
		)

		self.db_session = db_session
		self.config = config or WorkflowConfig.from_env()
		self.config.db_session = db_session

		# Initialize LLM
		self.llm = ChatGoogleGenerativeAI(model=self.config.model_name, temperature=self.config.temperature)

		# Initialize services
		self._init_global_kb_service()
		self._init_guardrails()

		# Build workflow
		self.compiled_graph = self._build_workflow()

		logger.success(
			'ðŸš€ Simplified Workflow initialized successfully',
			initialization_time=time.time() - self.start_time,
		)

	def _init_global_kb_service(self):
		"""Initialize Global Knowledge Base Service"""
		try:
			from app.modules.agentic_rag.services.global_kb_service import (
				GlobalKBService,
			)

			self.global_kb_service = GlobalKBService(self.db_session)
			logger.info('Global KB Service initialized successfully')
		except Exception as e:
			logger.error(f'Error initializing Global KB Service: {str(e)}')
			self.global_kb_service = None

	def _init_guardrails(self):
		"""Initialize Guardrail System"""
		try:
			guardrail_config = {
				'enable_input_guardrails': True,
				'enable_output_guardrails': True,
				'max_input_length': 5000,
				'strict_mode': False,
			}
			self.guardrail_manager = ChatWorkflowGuardrailManager(guardrail_config)
			logger.info('Guardrail system initialized successfully')
		except Exception as e:
			logger.error(f'Error initializing guardrails: {str(e)}')
			self.guardrail_manager = None

	def _build_workflow(self) -> StateGraph:
		"""Build simplified workflow vá»›i dual RAG máº·c Ä‘á»‹nh"""
		workflow = StateGraph(AgentState)

		# Get tools
		tools = get_tools(self.config)
		tool_node = ToolNode(tools)

		# Create wrapper functions for nodes
		async def router_wrapper(state, config=None):
			return await self._router_node(state, config or {})

		async def rag_query_wrapper(state, config=None):
			return await self._rag_query_node(state, config or {})

		async def agent_wrapper(state, config=None):
			return await self._agent_node(state, config or {})

		async def response_wrapper(state, config=None):
			return await self._response_generator_node(state, config or {})

		async def tools_wrapper(state, config=None):
			return await self._tools_node(state, config or {}, tool_node)

		def route_wrapper(state):
			return self._route_decision(state)

		def should_continue_wrapper(state):
			return self._should_continue(state)

		# Add simplified nodes
		workflow.add_node('router', router_wrapper)
		workflow.add_node('rag_query', rag_query_wrapper)
		workflow.add_node('agent', agent_wrapper)
		workflow.add_node('tools', tools_wrapper)
		workflow.add_node('response_generator', response_wrapper)

		# Set entry point
		workflow.set_entry_point('router')

		# Simplified routing: router â†’ (rag_query | agent)
		workflow.add_conditional_edges(
			'router',
			route_wrapper,
			{
				'rag_query': 'rag_query',
				'agent': 'agent',
			},
		)

		# Simple flow: rag_query â†’ response_generator
		workflow.add_edge('rag_query', 'response_generator')

		# Agent flow: agent â†’ (tools | END)
		workflow.add_conditional_edges('agent', should_continue_wrapper, {'tools': 'tools', END: END})
		workflow.add_edge('tools', 'agent')

		# Response generator â†’ END
		workflow.add_edge('response_generator', END)

		# Compile with memory
		checkpointer = MemorySaver()
		return workflow.compile(checkpointer=checkpointer)

	async def _router_node(self, state: AgentState, config: Dict[str, Any]) -> AgentState:
		"""Router Node vá»›i Intelligent routing + Guardrails"""
		start_time = time.time()
		thread_id = config.get('configurable', {}).get('thread_id', 'unknown')

		logger.workflow_start(
			'Router Node - Intelligent Query Routing with Guardrails',
			thread_id=thread_id,
			router_enabled=True,
			guardrails_enabled=True,
		)

		# Get user message
		messages = state.get('messages', [])
		if not messages:
			return {
				**state,
				'router_decision': {
					'target': 'general',
					'explanation': 'No user input',
				},
			}

		# Extract user query
		user_query = None
		for msg in reversed(messages):
			if hasattr(msg, 'content') and msg.content:
				user_query = msg.content
				break

		if not user_query:
			return {
				**state,
				'router_decision': {'target': 'general', 'explanation': 'Empty query'},
			}

		# Apply guardrails
		if self.guardrail_manager:
			try:
				guardrail_context = {
					'thread_id': thread_id,
					'user_id': config.get('configurable', {}).get('user_id', 'unknown'),
					'conversation_step': 'router_input',
				}

				guardrail_result = self.guardrail_manager.check_user_input(user_query, guardrail_context)

				if not guardrail_result.passed:
					logger.error('Input BLOCKED by guardrails')
					return {
						**state,
						'router_decision': {
							'target': 'general',
							'explanation': 'Input blocked by content safety guardrails',
						},
						'guardrail_blocked': True,
					}

				if guardrail_result.modified_content:
					user_query = guardrail_result.modified_content

			except Exception as e:
				logger.error(f'Guardrail check failed: {str(e)}')

		# Route decision using LLM
		try:
			router_prompt = ChatPromptTemplate.from_messages([
				('system', ROUTER_SYSTEM_PROMPT),
				(
					'human',
					'User query: {input}\n\nAnalyze and determine the best routing target.',
				),
			])

			router_chain = router_prompt | self.llm.with_structured_output(RouterDecision)
			router_result = await router_chain.ainvoke({'input': user_query})

			target = router_result.target if hasattr(router_result, 'target') else 'general'
			explanation = router_result.explanation if hasattr(router_result, 'explanation') else 'Default routing'

			logger.info(f'ðŸŽ¯ Router Decision: {target} - {explanation}')

			processing_time = time.time() - start_time
			logger.info(
				'Router Node - Intelligent Query Routing with Guardrails',
				processing_time,
				target_selected=target,
			)

			return {
				**state,
				'router_decision': {'target': target, 'explanation': explanation},
				'routing_complete': True,
			}

		except Exception as e:
			logger.error(f'Router failed: {str(e)}')
			return {
				**state,
				'router_decision': {
					'target': 'general',
					'explanation': f'Router error: {str(e)[:100]}',
				},
				'routing_complete': True,
			}

	async def _rag_query_node(self, state: AgentState, config: Dict[str, Any]) -> AgentState:
		"""RAG Query Node - Always use Dual RAG (Global KB + Conversation KB)"""
		start_time = time.time()
		thread_id = config.get('configurable', {}).get('thread_id', 'unknown')

		logger.workflow_start('RAG Query Node - Dual RAG Retrieval', thread_id=thread_id)

		messages = state.get('messages', [])
		if not messages:
			return state

		# Get user query
		user_query = None
		for msg in reversed(messages):
			if hasattr(msg, 'content') and msg.content:
				user_query = msg.content
				break

		if not user_query:
			return state

		try:
			# Always use dual RAG approach
			conv_context = ''
			global_context = ''

			# 1. Search Conversation KB
			try:
				collection_id = f'conversation_{thread_id}'
				logger.info(f'ðŸ” Searching conversation KB: {collection_id}')

				from app.modules.agentic_rag.agent.rag_graph import RAGAgentGraph
				from app.modules.agentic_rag.repository.kb_repo import KBRepository

				kb_repo = KBRepository(collection_name=collection_id)
				rag_agent = RAGAgentGraph(kb_repo=kb_repo, collection_id=collection_id)
				conv_result = await rag_agent.answer_query(query=user_query, collection_id=collection_id)

				conv_answer = conv_result.get('answer', '')
				conv_sources = conv_result.get('sources', [])

				if conv_answer:
					conv_context = f'ðŸ“ Conversation Knowledge:\n{conv_answer}'
					if conv_sources:
						conv_context += '\nðŸ“š Sources:'
						for i, source in enumerate(conv_sources, 1):
							source_info = f'\n{i}. Document ID: {source.get("id", "Unknown")}'
							if 'metadata' in source and 'source' in source['metadata']:
								source_info += f' (File: {source["metadata"]["source"]})'
							conv_context += source_info

			except Exception as e:
				logger.warning(f'Conversation KB search failed: {str(e)}')

			# 2. Search Global KB
			if self.global_kb_service:
				try:
					logger.info('ðŸŒ Searching global KB')
					global_results = await self.global_kb_service.search_global_knowledge(user_query, top_k=3)
					logger.debug(f'[DEBUG] Global KB search results: {global_results}')
					if global_results:
						# Combine top results into a single context string
						contexts = []
						for i, item in enumerate(global_results, 1):
							content = item.get('content', '')
							title = item.get('metadata', {}).get('title', '')
							source = item.get('metadata', {}).get('source', '')
							if title or source:
								contexts.append(f'{i}. {title}\n{content}\n(Source: {source})')
							else:
								contexts.append(f'{i}. {content}')
						global_context = 'ðŸŒ Global Knowledge:\n' + '\n\n'.join(contexts)
				except Exception as e:
					logger.warning(f'Global KB search failed: {str(e)}')

			# 3. Combine both contexts
			combined_context = ''
			if conv_context and global_context:
				combined_context = f'{conv_context}\n\n{global_context}'
			elif conv_context:
				combined_context = conv_context
			elif global_context:
				combined_context = global_context

			processing_time = time.time() - start_time
			logger.info(
				'RAG Query Node - Dual RAG Retrieval',
				processing_time,
				context_retrieved=bool(combined_context),
				conv_context_found=bool(conv_context),
				global_context_found=bool(global_context),
			)

			return {
				**state,
				'combined_rag_context': combined_context,
				'rag_used': True,
				'retrieval_quality': 'good' if combined_context else 'no_results',
			}

		except Exception as e:
			logger.error(f'Dual RAG Query failed: {str(e)}')
			return {
				**state,
				'combined_rag_context': '',
				'rag_used': False,
				'retrieval_quality': 'error',
			}

	async def _agent_node(self, state: AgentState, config: Dict[str, Any]) -> AgentState:
		"""Main Agent Node vá»›i RAG context vÃ  tools"""
		start_time = time.time()
		thread_id = config.get('configurable', {}).get('thread_id', 'unknown')

		logger.workflow_start(
			'Agent Node - Natural Model Invocation',
			thread_id=thread_id,
			has_context=bool(state.get('combined_rag_context')),
		)

		# Get system prompt with persona support		system_prompt = DEFAULT_SYSTEM_PROMPT
		if self.config and self.config.persona_enabled:
			persona_prompt = self.config.get_persona_prompt()
			if persona_prompt:
				system_prompt = persona_prompt  # Simple system prompt without forced tool instructions
		enhanced_system = f"""{system_prompt}

ENTERVIEW AI ASSISTANT - SESSION: {thread_id}
"""

		# Add RAG context if available
		combined_context = state.get('combined_rag_context')
		if combined_context:
			enhanced_system += f'\nï¿½ AVAILABLE CONTEXT:\n{combined_context[:800]}...\n'  # Prepare messages
		messages = state.get('messages', [])
		if not messages:
			return {'messages': [SystemMessage(content=enhanced_system)]}  # No forced tool calling - let the model decide naturally

		# Create prompt
		prompt = ChatPromptTemplate.from_messages([
			('system', enhanced_system),
			MessagesPlaceholder(variable_name='chat_history'),
			MessagesPlaceholder(variable_name='agent_scratchpad'),
		])
		formatted_prompt = prompt.format_messages(
			chat_history=messages,
			agent_scratchpad=[],
		)  # Model will naturally decide whether to use tools or not
		tool_definitions = get_tool_definitions(self.config)
		model_with_tools = self.llm.bind_tools(tool_definitions)

		# Natural model invocation - completely autonomous tool decision
		response = await model_with_tools.ainvoke(
			formatted_prompt,
			{
				'system_time': datetime.now(tz=timezone.utc).isoformat(),
				'unified_mode': True,
				'conversation_id': thread_id,
			},
		)

		# Log tool calls
		if hasattr(response, 'tool_calls') and response.tool_calls:
			logger.info(f'ðŸ”§ Agent will execute {len(response.tool_calls)} tool calls')
			for i, tool_call in enumerate(response.tool_calls, 1):
				tool_name = tool_call.get('name', 'unknown_tool')
				logger.info(f'ðŸ”§ Tool Call #{i}: {tool_name}')
		else:
			logger.info('ðŸ’¬ Agent generated text response without tool calls')

		processing_time = time.time() - start_time
		logger.info(
			'Agent Node - Model Invocation',
			processing_time,
			response_length=len(str(response.content)),
		)

		return {**state, 'messages': messages + [response]}

	async def _response_generator_node(self, state: AgentState, config: Dict[str, Any]) -> AgentState:
		"""Generate final response vá»›i RAG context"""
		logger.info('ðŸ“ Response generator executing...')

		try:
			messages = state.get('messages', [])
			combined_context = state.get('combined_rag_context', '')

			# Build enhanced prompt
			if combined_context:
				system_prompt = f"""Báº¡n lÃ  AI assistant thÃ´ng minh vá»›i kháº£ nÄƒng truy cáº­p kiáº¿n thá»©c tá»« nhiá»u nguá»“n.

CONTEXT Tá»ª KNOWLEDGE BASES:
{combined_context}

HÆ¯á»šNG DáºªN:
- Sá»­ dá»¥ng thÃ´ng tin tá»« context Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i
- Náº¿u context khÃ´ng Ä‘á»§, hÃ£y thá»«a nháº­n vÃ  Ä‘Æ°a ra cÃ¢u tráº£ lá»i tá»‘t nháº¥t cÃ³ thá»ƒ
- LuÃ´n cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c vÃ  há»¯u Ã­ch
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t trá»« khi Ä‘Æ°á»£c yÃªu cáº§u khÃ¡c
- Ghi rÃµ nguá»“n thÃ´ng tin khi cáº§n thiáº¿t
"""
			else:
				system_prompt = DEFAULT_SYSTEM_PROMPT

			# Generate response
			enhanced_messages = [SystemMessage(content=system_prompt)] + messages
			response = await self.llm.ainvoke(enhanced_messages)

			# Add metadata
			routing_info = state.get('rag_routing', {})
			if routing_info:
				metadata_note = f'\n\n_[Sá»­ dá»¥ng {routing_info} knowledge base(s)]_'
				response.content += metadata_note

			return {
				**state,
				'messages': messages + [response],
				'current_node': 'response_generator',
			}

		except Exception as e:
			logger.error(f'Response generator error: {str(e)}')
			error_response = AIMessage(content=f'Xin lá»—i, cÃ³ lá»—i trong quÃ¡ trÃ¬nh táº¡o pháº£n há»“i: {str(e)}')
			return {**state, 'messages': state.get('messages', []) + [error_response]}

	async def _tools_node(self, state: AgentState, config: Dict[str, Any], tool_node) -> AgentState:
		"""Simplified Tools Node"""
		thread_id = config.get('configurable', {}).get('thread_id', 'unknown')

		logger.info(f'ðŸ”§ [Tools] Starting tool execution for thread: {thread_id}')

		# Get messages and check for tool calls
		messages = state.get('messages', [])
		if not messages:
			logger.warning('ðŸ”§ [Tools] No messages found')
			return state

		last_message = messages[-1]
		if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
			logger.warning('ðŸ”§ [Tools] No tool calls found')
			return state

		# Log tool calls
		for i, tool_call in enumerate(last_message.tool_calls, 1):
			tool_name = tool_call.get('name', 'unknown_tool')
			logger.info(f'ðŸ”§ [Tools] Executing Tool #{i}: {tool_name}')

		try:
			# Execute tools using the tool node
			result_state = await tool_node.ainvoke(state, config)

			logger.info(f'ðŸ”§ [Tools] All tools executed successfully')
			return result_state

		except Exception as e:
			logger.error(f'ðŸ”§ [Tools] Tool execution failed: {str(e)}')
			# Return state with error info
			return {
				**state,
				'tool_execution_error': str(e),
				'tool_execution_failed': True,
			}

	def _route_decision(self, state: AgentState) -> str:
		"""Simplified route decision logic"""
		router_decision = state.get('router_decision', {})
		target = router_decision.get('target', 'agent') if isinstance(router_decision, dict) else 'agent'

		# Simple mapping
		if target == 'rag_query':
			actual_target = 'rag_query'
		else:
			actual_target = 'agent'
		logger.info(f'ðŸ”€ Routing Decision: {target} â†’ {actual_target}')
		logger.info(f'ðŸ“ Routing Explanation: {router_decision.get("explanation", "No explanation provided")}')

		return actual_target

	def _should_continue(self, state: AgentState) -> str:
		"""Determine if agent should continue with tools or end"""
		messages = state.get('messages', [])
		if not messages:
			logger.info('ðŸ”š No messages found - ending workflow')
			return END

		last_message = messages[-1]
		if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
			logger.info('ðŸ”š No tool calls detected - ending workflow')
			return END
		else:
			tool_count = len(last_message.tool_calls)
			tool_names = [tc.get('name', 'unknown') for tc in last_message.tool_calls]
			logger.info(
				f'ðŸ”§ Tool calls detected - continuing to tools',
				tool_count=tool_count,
				tools=tool_names,
			)
			return 'tools'

	async def process_message(
		self,
		user_message: str,
		user_id: Optional[str] = None,
		conversation_id: Optional[str] = None,
		config_override: Optional[Dict[str, Any]] = None,
	) -> Dict[str, Any]:
		"""Process message vá»›i unified workflow"""
		start_time = time.time()
		session_id = conversation_id or f'session_{int(time.time())}'

		logger.workflow_start(
			'Simplified Workflow Message Processing',
			user_id=user_id,
			conversation_id=conversation_id,
			message_length=len(user_message),
		)

		try:
			# Create initial state
			initial_state = StateManager.create_initial_state(user_message=user_message, user_id=user_id, session_id=session_id)

			# Add conversation_id to metadata
			if conversation_id:
				initial_state['conversation_metadata']['conversation_id'] = conversation_id

			# Prepare config
			runtime_config = {
				'configurable': {
					'thread_id': session_id,
					**self.config.to_dict(),
				}
			}

			if config_override:
				runtime_config['configurable'].update(config_override)

			# Execute workflow
			final_state = await self.compiled_graph.ainvoke(initial_state, config=runtime_config)

			# Extract response
			response = self._extract_response(final_state)
			processing_time = time.time() - start_time

			logger.success(
				'Simplified Workflow processing completed',
				processing_time=processing_time,
				response_length=len(response),
				features_used={
					'router': final_state.get('routing_complete', False),
					'rag': bool(final_state.get('combined_rag_context')),
					'guardrails': not final_state.get('guardrail_blocked', False),
					'tools': final_state.get('tool_execution_failed', False) == False and 'tool_execution_error' not in final_state,
				},
			)

			# Log tool usage summary if tools were used
			if 'tool_execution_error' not in final_state and not final_state.get('tool_execution_failed', False):
				messages = final_state.get('messages', [])
				tool_calls_count = 0
				tools_used = []

				for msg in messages:
					if hasattr(msg, 'tool_calls') and msg.tool_calls:
						tool_calls_count += len(msg.tool_calls)
						for tc in msg.tool_calls:
							tool_name = tc.get('name', 'unknown')
							if tool_name not in tools_used:
								tools_used.append(tool_name)

				if tool_calls_count > 0:
					logger.success(
						f'ðŸ”§ Workflow completed with {tool_calls_count} tool calls',
						tools_used=tools_used,
					)

			# Log comprehensive workflow summary
			self._log_workflow_summary(final_state, processing_time)

			return {
				'response': response,
				'state': final_state,
				'metadata': {
					'processing_time': processing_time,
					'workflow_type': 'simplified',
					'router_decision': final_state.get('router_decision'),
					'rag_used': bool(final_state.get('combined_rag_context')),
					'guardrails_passed': not final_state.get('guardrail_blocked', False),
				},
			}
		except Exception as e:
			logger.error(f'Simplified workflow error: {str(e)}')
			logger.error(f'Error type: {type(e).__name__}')
			logger.error(f'Error occurred at processing time: {time.time() - start_time:.2f}s')

			return {
				'response': f'Xin lá»—i, cÃ³ lá»—i xáº£y ra trong quÃ¡ trÃ¬nh xá»­ lÃ½: {str(e)}',
				'state': {},
				'metadata': {
					'processing_time': time.time() - start_time,
					'error': str(e),
					'error_type': type(e).__name__,
					'workflow_type': 'simplified',
				},
			}

	def _extract_response(self, final_state: Dict[str, Any]) -> str:
		"""Extract response from final state"""
		messages = final_state.get('messages', [])
		if messages:
			last_message = messages[-1]
			return last_message.content if hasattr(last_message, 'content') else str(last_message)
		return 'KhÃ´ng cÃ³ pháº£n há»“i Ä‘Æ°á»£c táº¡o.'

	def _log_workflow_summary(self, final_state: Dict[str, Any], processing_time: float):
		"""Simple workflow summary"""
		logger.info('=' * 40)
		logger.info('ðŸŽ¯ WORKFLOW SUMMARY')
		logger.info('=' * 40)

		logger.info(f'â±ï¸  Processing Time: {processing_time:.2f}s')
		logger.info(f'ðŸ”€ Router Target: {final_state.get("router_decision", {}).get("target", "unknown")}')
		logger.info(f'ðŸ“š RAG Used: {bool(final_state.get("combined_rag_context"))}')

		# Count tools used
		messages = final_state.get('messages', [])
		tool_count = 0
		for msg in messages:
			if hasattr(msg, 'tool_calls') and msg.tool_calls:
				tool_count += len(msg.tool_calls)

		logger.info(f'ðŸ”§ Total Tool Calls: {tool_count}')

		# Check for errors
		if final_state.get('tool_execution_failed'):
			logger.warning('âš ï¸  Tool execution failed')
		if final_state.get('guardrail_blocked'):
			logger.warning('âš ï¸  Input was blocked by guardrails')

		logger.info('=' * 40)

	async def initialize_global_knowledge(self) -> Dict[str, Any]:
		"""Initialize Global Knowledge Base"""
		if not self.global_kb_service:
			return {'error': 'Global KB Service not available'}

		try:
			result = await self.global_kb_service.initialize_default_knowledge()
			logger.info('Global Knowledge Base initialized successfully')
			return result
		except Exception as e:
			logger.error(f'Error initializing global knowledge: {str(e)}')
			return {'error': str(e)}


# Factory functions
def create_workflow(db_session: Session, config: Optional[WorkflowConfig] = None) -> Workflow:
	"""Factory function Ä‘á»ƒ táº¡o simplified workflow"""
	return Workflow(db_session, config)


# Backward compatibility aliases
def create_unified_workflow(db_session: Session, config: Optional[WorkflowConfig] = None) -> Workflow:
	"""Alias for create_workflow - backward compatibility"""
	return create_workflow(db_session, config)


def create_agentic_rag_workflow(db_session: Session, config: Optional[WorkflowConfig] = None) -> Workflow:
	"""Alias for create_workflow - backward compatibility"""
	return create_workflow(db_session, config)


def create_enhanced_rag_workflow(db_session: Session, config: Optional[WorkflowConfig] = None) -> Workflow:
	"""Alias for create_workflow - backward compatibility"""
	return create_workflow(db_session, config)


def create_workflow_with_rag(db_session: Session, config: Optional[WorkflowConfig] = None) -> Workflow:
	"""Alias for create_workflow - backward compatibility"""
	return create_workflow(db_session, config)
